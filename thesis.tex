% !TeX spellcheck = en_GB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
%    Engineer thesis LaTeX template      %
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[a4paper,twoside,12pt]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[polish,british]{babel}
\usepackage{indentfirst}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage[page]{appendix}

\usepackage{setspace}
\onehalfspacing


\frenchspacing

\usepackage{listings}
\lstset{
	language={},
	basicstyle=\ttfamily,
	keywordstyle=\lst@ifdisplaystyle\color{blue}\fi,
	commentstyle=\color{gray}
}

%%%%%%%%%



%%%%%%%%%%%% FANCY HEADERS %%%%%%%%%%%%%%%

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\nouppercase{\it\rightmark}}
\fancyhead[RE]{\nouppercase{\it\leftmark}}
\fancyhead[LE,RO]{\it\thepage}


\fancypagestyle{onlyPageNumbers}{%
   \fancyhf{}
   \fancyhead[LE,RO]{\it\thepage}
}

\fancypagestyle{PageNumbersChapterTitles}{%
   \fancyhf{}
   \fancyhead[LO]{\nouppercase{\it\rightmark}}
   \fancyhead[RE]{\nouppercase{\it\leftmark}}
   \fancyhead[LE,RO]{\it\thepage}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
% listings
\usepackage{listings}
\lstset{%
language=C++,%
commentstyle=\textit,%
identifierstyle=\textsf,%
keywordstyle=\sffamily\bfseries, %\texttt, %
%captionpos=b,%
tabsize=3,%
frame=lines,%
numbers=left,%
numberstyle=\tiny,%
numbersep=5pt,%
breaklines=true,%
morekeywords={descriptor_gaussian,descriptor,partition,fcm_possibilistic,dataset,my_exception,exception,std,vector},%
escapeinside={@*}{*@},%
%texcl=true, % wylacza tryb verbatim w komentarzach jednolinijkowych
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% TODO LIST GENERATOR %%%%%%%%%

\usepackage{color}
\definecolor{brickred}      {cmyk}{0   , 0.89, 0.94, 0.28}

\makeatletter \newcommand \kslistofremarks{\section*{Remarks} \@starttoc{rks}}
  \newcommand\l@uwagas[2]
    {\par\noindent \textbf{#2:} %\parbox{10cm}
{#1}\par} \makeatother


\newcommand{\remark}[1]{%
{%\marginpar{\textdbend}
{\color{brickred}{[#1]}}}%
\addcontentsline{rks}{uwagas}{\protect{#1}}%
}

%%%%%%%%%%%%%% END OF TODO LIST GENERATOR %%%%%%%%%%%

% some issues...

\newcounter{PagesWithoutNumbers}

\newcommand{\hcancel}[1]{%
    \tikz[baseline=(tocancel.base)]{
        \node[inner sep=0pt,outer sep=0pt] (tocancel) {#1};
        \draw[red] (tocancel.south west) -- (tocancel.north east);
    }%
}%

\newcommand{\MonthName}{%
  \ifcase\the\month
  \or January% 1
  \or February% 2
  \or March% 3
  \or April% 4
  \or May% 5
  \or June% 6
  \or July% 7
  \or August% 8
  \or September% 9
  \or October% 10
  \or November% 11
  \or December% 12
  \fi}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Helvetica font macros for the title page:
\newcommand{\headerfont}{\fontfamily{phv}\fontsize{18}{18}\bfseries\scshape\selectfont}
\newcommand{\titlefont}{\fontfamily{phv}\fontsize{18}{18}\selectfont}
\newcommand{\otherfont}{\fontfamily{phv}\fontsize{14}{14}\selectfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\Author}{Damian Kucharski}
\newcommand{\Supervisor}{Łukasz Wróbel, PhD}
\newcommand{\Title}{Gradient Boosting Framework for classification data}
\newcommand{\Polsl}{Silesian University of Technology}
\newcommand{\Faculty}{Faculty of Automatic Control, Electronics and Computer Science}


\begin{document}
	
%%%%%%%%%%%%%%%%%%  Title page %%%%%%%%%%%%%%%%%%%
\pagestyle{empty}
{
	\newgeometry{top=2.5cm,%
	             bottom=2.5cm,%
	             left=3cm,
	             right=2.5cm}
	\sffamily
	\rule{0cm}{0cm}
	
	\begin{center}
	\includegraphics[width=29mm]{polsl}
	\end{center}
	\vspace{1cm}
	\begin{center}
	\headerfont \Polsl
	\end{center}
	\begin{center}
	\headerfont \Faculty
	\end{center}
	\vfill
	\begin{center}
	\titlefont Engineer  thesis
	\end{center}
	\vfill
	
	\begin{center}
	\otherfont \Title\par
	\end{center}
	
	\vfill
	
	\vfill
	
	\noindent\vbox
	{
		\hbox{\otherfont author: \Author}
		\vspace{12pt}
		\hbox{\otherfont supervisor: \Supervisor}
	}
	\vfill

   \begin{center}
   \otherfont Gliwice,  \MonthName\ \the\year
   \end{center}	
	\restoregeometry
}


\cleardoublepage


\rmfamily
\normalfont


%%%%%%%%%%%% statements required by law and Dean's office %%%%%%%%%%
\cleardoublepage

\begin{flushright}
załącznik nr 2 do zarz. nr 97/08/09
\end{flushright}

\vfill

\begin{center}
\Large\bfseries Oświadczenie
\end{center}

\vfill

Wyrażam  zgodę / Nie wyrażam zgody*  na  udostępnienie  mojej  pracy  dyplomowej / rozprawy doktorskiej*.

\vfill

Gliwice, dnia {\selectlanguage{polish}\today}

\vfill

\rule{0.5\textwidth}{0cm}\dotfill

\rule{0.5\textwidth}{0cm}
\begin{minipage}{0.45\textwidth}
{\begin{center}(podpis)\end{center}}
\end{minipage}

\vfill

\rule{0.5\textwidth}{0cm}\dotfill

\rule{0.5\textwidth}{0cm}
\begin{minipage}{0.45\textwidth}
{\begin{center}\rule{0mm}{5mm}(poświadczenie wiarygodności podpisu przez Dziekanat)\end{center}}
\end{minipage}


\vfill

* podkreślić właściwe




%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage

\rule{1cm}{0cm}

\vfill

\begin{center}
\Large\bfseries Oświadczenie promotora
\end{center}

\vfill

Oświadczam, że praca „\Title” spełnia wymagania formalne pracy dyplomowej inżynierskiej.

\vfill



\vfill

Gliwice, dnia {\selectlanguage{polish}\today}

\rule{0.5\textwidth}{0cm}\dotfill

\rule{0.5\textwidth}{0cm}
\begin{minipage}{0.45\textwidth}
{\begin{center}(podpis promotora)\end{center}}
\end{minipage}

\vfill



\cleardoublepage


%%%%%%%%%%%%%%%%%% Table of contents %%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{Roman}
\pagestyle{onlyPageNumbers}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{PagesWithoutNumbers}{\value{page}}
\mainmatter
\pagestyle{PageNumbersChapterTitles}

%%%%%%%%%%%%%% body of the thesis %%%%%%%%%%%%%%%%%


\chapter{Introduction}

\begin{itemize}
\item introduction into the problem domain
\item settling of the problem in the domain
\item objective of the thesis
\item scope of the thesis
\item short description of chapters
\item clear description of contribution of the thesis's author – in case of more authors table with enumeration of contribution of authors
\newline
------------------------------
\newline
The roots of artificial intelligence and machine learning appear quite early in history, one of earliest examples could be introduction of Bayes Theorem at 1700s, followed by regression analysis at 1800s. The field constantly evolved, but it got much more attention pretty recently when computational possibilities expanded enough for big data to be processed and more complex algorithms (like neural networks) to be used.
The thing that at first was called inferential statistics, data mining and recently data science and machine learning has application in almost any field imaginable. Tools such as spam filters, chatbots, web search results, analysis of medical imaging, reconstruction of images, text generation and autocompletion, robotics, video and board games, logistic processes optimization and much more - they all benefit from intelligent solutions that are built into them.
Machines are able to exceed performance of humans in many critical fields, well trained machine learning model can analyse MRI imaging as well as radiologist with 20 years of experience. Very recently, at November 2020, company called OpenAI, known for creation of AlphaZero - best chess playing program and AlphaGo - first ever program to beat human in the ancient chineese game of Go, has created program called AlphaFold2. It solves task of so called protein folding, a 50-year-old grand challenge in biology, with tremendous 90 percent accuracy, exceeding performance of its first version from 2 years before by over 30 percent. This work by some is named the biggest accomplishment of artificial intelligence studies and is predicted to be crucial part to next great discoveries in medical and biology fields. Some applications, such as the evolutionary analysis of proteins, are set to flourish because the tsunami of available genomic data might now be reliably translated into structures. This can for example help to prevent such events like covid-19 pandemic and help to find cure for many diseases whose mechanisms are not yet well understood.
\newline
Occurances like that show two things. First - that the field of machine learning and artificial intelligence has and will have great impact on the world, and second - that the advancements in this field are very far from slowing down, in fact before mentioned AlphaFold and other novel machine learning models like transformers that pushed natural language modeling to the whole new level are the best evidence that the opposite is happening - this growth is speeding up.
\newline
However with new ideas, growth of the availability of data and processing power, complexity of the models expanded and because of that it is now much harder to make them perform optimally. It is much harder to break into field now, than it was just few years ago. Ideas ones considered state-of-art are now mostly used only for educational purposes and even for experienced specialists it is much harder to create model that actually achieves it's full potential. The quality of final model of the data depends mostly on two factors: 
\item The quality and prepossessing of the data, feature engineering
\item Choosing right model and its hyperparameters.

Because of the reason of growing complexity of the modeling task the idea of automatic solution arose and is mostly referred to as Auto Modeling, Auto Machine Learning or just Auto-ML. Such a solution should automate at least one part of modeling pipeline, it means that it should either preprocess data for modeling purposes, select right model and optimize it's hyperparameter choices, or to do both.
Such a solution gives good baseline and push to the right direction, saving time otherwise spent for experiments that are destined to fail. In short words, Auto-ML is the idea to incorporate machine learning to create machine learning models. It is still rather new field, however it already gives promising results and likely in short amount of time it will completely replace big chunk of traditional work performed by data scientists and other practitioner in the field, just as spreadsheets replaced need for manual calculation on the paper for most of the use cases.
\newline
There are many approaches to solving such a problem, especially in terms of optimizing for model choice. 
Usually the hardest part of the task it to optimize hyperparameters. Most of them are different for each model type and while their optimal choice is crucial for good modeling of given data they can't be directly learned from it. Let's analyze simple example of probabilistic model that decides whether a person will be accepted to college given results of maths and physics test. Example of hyperparameter for such a model could be for example the way it measures its error which is often named cost function, or objective function. It cannot be optimized during model training as it is by itself used to optimize the model. It has to be selected beforehand and is constant for whole optimization procedure.
Another example of hyper parameter is not directly model specific but necessary for the model to learn - namely optimization method. What is important is that this method by itself can have it's own hyperparameters. More details on different hyper parameters will be explored later in the work but for now what is important is that the more complex the model is, the more hyper parameters which usually means much harder procedure of finding their optimal values.
\newline
The objective of this thesis is development of Auto-ML functionality in form of python library. It covers both data processing and model selection. Models built by it are based on gradient boosted trees algorithms that are optimized using users method of choice, implemented inside library. Namely, grid search and Bayesian optimization are supported. Apart from that, web based application was developed that can be accessed using internet or run on ones own machine. It allows for training model and viewing visualizations summarizing training and explaining models decisions. As with growing complexity of modern machine learning algorithms it is increasingly hard to understand their decisions and it is very common to just treat them as black box. However good understanding of model inner workings has proved in the past to be crucial in further enhancement of its performance. Good example can be paper "Visualizing and Understanding Convolutional Networks" (Trzeba to jakoś ładnie zacytować)[https://arxiv.org/abs/1311.2901] by Matthew D Zeiler et. al. This study analyzed winning convolutional neural network architecture of previous ImageNet competition. In result the next iteration of this competition has been won by this team of researchers.
Because of that software prepared to accompany this work has been designed to offer tools that make it easy to understand how model actually works to  make further research and domain knowledge application as simple as possible.
The library also offers lower level interface, that gives possibility to quickly ensemble machine learning pipelines for programmers without sacrificing complexity of it. Because of that it is much simpler to perform repetitive, standard tasks in data processing and modeling, at the same time enhancing simplicity and readability of the code. Hyperparametrs of such models can be then optimized with before mentioned methods.
The project was co-created with Arkadiusz Czerwiński who also ensured that regression tasks are well covered, whilst my scope was to prepare library for handling classification problems. Most of the work however was done in collaboration as big chunk of whole code was needed to handle both of use cases.


\end{itemize}

\chapter{[Problem analysis]}

\begin{itemize}
\item  problem analysis
\item state of the art, problem statement
\item  literature research (all sources in the thesis have to be referenced \cite{bib:article,bib:book,bib:conference,bib:internet})
\item description of existing solutions (also scientific ones, if the problem is scientifically researched), algorithms,  location of the thesis in the scientific domain
\end{itemize}

As was mentioned in the introduction, Auto-ML library that is subject of this thesis has to do both things - preprocess the data and optimize hyperparameters. Created model should perform classification tasks with high efficiency, measured using metrics such as accuracy and f1-score.
After training process it should be possible to plot information concerning effectiveness of training and model characteristics explanation.
All of these problems will be now explained so the solution presented by this thesis will be more understandable.

\subsection{Classification}

The main problem that the solution should solve is so called classification task. Such task consists of predicting fixed class label for given set of data. Classification is most the frequent problem in machine learning use cases and can be performed on any kind of input data. There are two types of classification problems - binary classification and multilabel classification.
In the first case model, given the data, has to choose one of only two classes, usually representing True/False values or presence/absence of something. Examples include classifying e-mails as spam and not-spam or predicting whether some person has some medical condition or not.
In the second case there are at least 3 labels, usually predefined, and model has to choose one of them that most likely categorizes data it was presented. Depending on the algorithm used to perform the task, it can be either split into multiple binary classification problems using so-called one-vs-all approach in which for each class individual problem of classifying if given data point belongs to given class of not. However some algorithms allow to perform multilabel classification at one step. Example of such problems is for example famous MNIST classification task, in which given picture of handwritten digit, program has to output what digit is present in the image.
To illustrate such a problem simple example will be provided. The problem to be solved will be classification of student college acceptance given scores of two tests. Data is presented on the following graph.

\subsection{Decision boundary}

Solving classification task reduces to find so called decision boundary. This term basically means that what has to be found are specific values for which divide the space of all possible values into regions representing specific label. In case of binary classification this space is divided into two regions.
As can be seen on the figure, high scores from both tests are needed for a student to be accepted.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.75]{college.png}
    \caption{Example of classification task}
    \label{fig:mesh1}
\end{figure}




In case of this dataset there are only two explanatory variables so the problem can be easily visualized. However number of these variables and therefore dimension of he input data is not limited in any way. I general solving classification task means finding N - 1 dimensional hyperplane (in case of 2-dimensional input such a hyperplane is just 1-dimensional line) that seperates datapoints of different classes.

\subsubsection{Linear and Nonlinear Boundaries}

Depending on the model type, decision boundary it generates can be different. The simplest case of decision boundary is linear decision boundary created as linear combination of the input features. Very often however such a model doesn't explain relationships between features and labels very well. In such a case non-linear decision boundary is needed. Different models have different ways to provide such a non-linear boundary. With such a boundaries however the problem of modeling too complex relationships arises, making a model very good at modeling data that was presented to it but failing to model new, unseen data.
Such a problems are called respectively under-fitting and over-fitting. When model under-fits the data it is said to have high bias. When it over-fits the data it is said to have high variance. The challange of finding model that is complex enough to model the data well without simultaneously modeling noise is called bias-variance trade-off. It will be adressed later along with methods to tackle it. Below figures showing different decision boundaries are presented to illustrate the problem. 


\subsection{Logistic Regression}






\chapter{Requirements and tools}

\begin{itemize}
\item functional and nonfunctional requirements
\item use cases (UML diagrams)
\item description of tools
\item methodology of design and implementation
\end{itemize}


\chapter{External specification}
\begin{itemize}
\item hardware and software requirements
\item installation procedure
\item activation procedure
\item types of users
\item user manual
\item system administration
\item security issues
\item example of usage
\item working scenarios (with screenshots or output files)
\end{itemize}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    y tick label style={
        /pgf/number format/.cd,
            fixed,
            fixed zerofill, % 1.0 instead of 1
            precision=1,
        /tikz/.cd
    },
    x tick label style={
        /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=2,
        /tikz/.cd
    }
]
\addplot [domain=0.0:0.1] {rnd};
\end{axis}
\end{tikzpicture}
\caption{A caption of a figure is \textbf{below} it.}
\label{fig:2}
\end{figure}


\chapter{Internal specification}

\begin{itemize}
\item concept of the system
\item system architecture
\item description of data structures (and data bases)
\item components, modules, libraries, resume of important classes (if used)
\item resume of important algorithms (if used)
\item details of implementation of selected parts
\item applied design patterns
\item UML diagrams
\end{itemize}


Use special environment for inline code, eg \lstinline|descriptor| or \lstinline|descriptor_gaussian|.
Longer parts of code put in the figure environment, eg. code in Fig. \ref{fig:pseudokod}. Very long listings–move to an appendix.

\begin{figure}
\centering
\begin{lstlisting}
class descriptor_gaussian : virtual public descriptor
{
   protected:
      /** core of the gaussian fuzzy set */
      double _mean;
      /** fuzzyfication of the gaussian fuzzy set */
      double _stddev;

   public:
      /** @param mean core of the set
          @param stddev standard deviation */
      descriptor_gaussian (double mean, double stddev);
      descriptor_gaussian (const descriptor_gaussian & w);
      virtual ~descriptor_gaussian();
      virtual descriptor * clone () const;

      /** The method elaborates membership to the gaussian fuzzy set. */
      virtual double getMembership (double x) const;

};
\end{lstlisting}
\caption{The \lstinline|descriptor_gaussian| class.}
\label{fig:pseudokod}
\end{figure}


\chapter{Verification and validation}
\begin{itemize}
\item testing paradigm (eg V model)
\item test cases, testing scope (full / partial)
\item detected and fixed bugs
\item results of experiments (optional)
\end{itemize}




\chapter{Conclusions}
\begin{itemize}
\item achieved results with regard to objectives of the thesis and requirements
\item path of further development (eg functional extension …)
\item encountered difficulties and problems
\end{itemize}


\begin{table}
\centering
\caption{A caption of a table is \textbf{above} it.}
\label{id:tab:wyniki}
\begin{tabular}{rrrrrrrr}
\toprule
	         &                                     \multicolumn{7}{c}{method}                                      \\
	         \cmidrule{2-8}
	         &         &         &        \multicolumn{3}{c}{alg. 3}        & \multicolumn{2}{c}{alg. 4, $\gamma = 2$} \\
	         \cmidrule(r){4-6}\cmidrule(r){7-8}
	$\zeta$ &     alg. 1 &   alg. 2 & $\alpha= 1.5$ & $\alpha= 2$ & $\alpha= 3$ &   $\beta = 0.1$  &   $\beta = -0.1$ \\
\midrule
	       0 &  8.3250 & 1.45305 &       7.5791 &    14.8517 &    20.0028 & 1.16396 &                       1.1365 \\
	       5 &  0.6111 & 2.27126 &       6.9952 &    13.8560 &    18.6064 & 1.18659 &                       1.1630 \\
	      10 & 11.6126 & 2.69218 &       6.2520 &    12.5202 &    16.8278 & 1.23180 &                       1.2045 \\
	      15 &  0.5665 & 2.95046 &       5.7753 &    11.4588 &    15.4837 & 1.25131 &                       1.2614 \\
	      20 & 15.8728 & 3.07225 &       5.3071 &    10.3935 &    13.8738 & 1.25307 &                       1.2217 \\
	      25 &  0.9791 & 3.19034 &       5.4575 &     9.9533 &    13.0721 & 1.27104 &                       1.2640 \\
	      30 &  2.0228 & 3.27474 &       5.7461 &     9.7164 &    12.2637 & 1.33404 &                       1.3209 \\
	      35 & 13.4210 & 3.36086 &       6.6735 &    10.0442 &    12.0270 & 1.35385 &                       1.3059 \\
	      40 & 13.2226 & 3.36420 &       7.7248 &    10.4495 &    12.0379 & 1.34919 &                       1.2768 \\
	      45 & 12.8445 & 3.47436 &       8.5539 &    10.8552 &    12.2773 & 1.42303 &                       1.4362 \\
	      50 & 12.9245 & 3.58228 &       9.2702 &    11.2183 &    12.3990 & 1.40922 &                       1.3724 \\
\bottomrule
\end{tabular}
\end{table}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter
\pagenumbering{Roman}
\stepcounter{PagesWithoutNumbers}
\setcounter{page}{\value{PagesWithoutNumbers}}

\pagestyle{onlyPageNumbers}

%%%%%%%%%%% bibliography %%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{bibliography}

%%%%%%%%%  appendices %%%%%%%%%%%%%%%%%%%

\begin{appendices}




\chapter*{List of abbreviations and symbols}

\begin{itemize}
\item[DNA] deoxyribonucleic acid
\item[MVC] model--view--controller
\item[$N$] cardinality of data set
\item[$\mu$] membership function of a fuzzy set
\item[$\mathbb{E}$] set of edges of a graph
\item[$\mathcal{L}$] Laplace transformation
\end{itemize}


\chapter*{Listings}

(Put long listings in the appendix.)

\begin{lstlisting}
partition fcm_possibilistic::doPartition
                             (const dataset & ds)
{
   try
   {
      if (_nClusters < 1)
         throw std::string ("unknown number of clusters");
      if (_nIterations < 1 and _epsilon < 0)
         throw std::string ("You should set a maximal number of iteration or minimal difference -- epsilon.");
      if (_nIterations > 0 and _epsilon > 0)
         throw std::string ("Both number of iterations and minimal epsilon set -- you should set either number of iterations or minimal epsilon.");

      auto mX = ds.getMatrix();
      std::size_t nAttr = ds.getNumberOfAttributes();
      std::size_t nX    = ds.getNumberOfData();
      std::vector<std::vector<double>> mV;
      mU = std::vector<std::vector<double>> (_nClusters);
      for (auto & u : mU)
         u = std::vector<double> (nX);
      randomise(mU);
      normaliseByColumns(mU);
      calculateEtas(_nClusters, nX, ds);
      if (_nIterations > 0)
      {
         for (int iter = 0; iter < _nIterations; iter++)
         {
            mV = calculateClusterCentres(mU, mX);
            mU = modifyPartitionMatrix (mV, mX);
         }
      }
      else if (_epsilon > 0)
      {
         double frob;
         do
         {
            mV = calculateClusterCentres(mU, mX);
            auto mUnew = modifyPartitionMatrix (mV, mX);

            frob = Frobenius_norm_of_difference (mU, mUnew);
            mU = mUnew;
         } while (frob > _epsilon);
      }
      mV = calculateClusterCentres(mU, mX);
      std::vector<std::vector<double>> mS = calculateClusterFuzzification(mU, mV, mX);

      partition part;
      for (int c = 0; c < _nClusters; c++)
      {
         cluster cl;
         for (std::size_t a = 0; a < nAttr; a++)
         {
            descriptor_gaussian d (mV[c][a], mS[c][a]);
            cl.addDescriptor(d);
         }
         part.addCluster(cl);
      }
      return part;
   }
   catch (my_exception & ex)
   {
      throw my_exception (__FILE__, __FUNCTION__, __LINE__, ex.what());
   }
   catch (std::exception & ex)
   {
      throw my_exceptionn (__FILE__, __FUNCTION__, __LINE__, ex.what());
   }
   catch (std::string & ex)
   {
      throw my_exception (__FILE__, __FUNCTION__, __LINE__, ex);
   }
   catch (...)
   {
      throw my_exception (__FILE__, __FUNCTION__, __LINE__, "unknown expection");
   }
}
\end{lstlisting}

\chapter*{Contents of attached CD}

The thesis is accompanied by a CD containing:
\begin{itemize}
\item thesis (\LaTeX\ source files and final \texttt{pdf} file),
\item source code of the application,
\item test data.
\end{itemize}


\listoffigures
\listoftables
	
\end{appendices}


\end{document}


%% Finis coronat opus. 
