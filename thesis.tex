% !TeX spellcheck = en_GB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
%    Engineer thesis LaTeX template      %
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[a4paper,twoside,12pt]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[polish,british]{babel}
\usepackage{indentfirst}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage[page]{appendix}
\usepackage{setspace}
\onehalfspacing
\usepackage{caption}
\DeclareCaptionFormat{citation}{%
   \ifx\captioncitation\relax\relax\else
     \captioncitation\par
   \fi
   #1#2#3\par}
\newcommand*\setcaptioncitation[1]{\def\captioncitation{\textit{Source:}~#1}}
\let\captioncitation\relax
\captionsetup{format=citation,justification=centering}

\frenchspacing

\usepackage{listings}
\lstset{
	language={},
	basicstyle=\ttfamily,
	keywordstyle=\lst@ifdisplaystyle\color{blue}\fi,
	commentstyle=\color{gray}
}

%%%%%%%%%



%%%%%%%%%%%% FANCY HEADERS %%%%%%%%%%%%%%%

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\nouppercase{\it\rightmark}}
\fancyhead[RE]{\nouppercase{\it\leftmark}}
\fancyhead[LE,RO]{\it\thepage}


\fancypagestyle{onlyPageNumbers}{%
   \fancyhf{}
   \fancyhead[LE,RO]{\it\thepage}
}

\fancypagestyle{PageNumbersChapterTitles}{%
   \fancyhf{}
   \fancyhead[LO]{\nouppercase{\it\rightmark}}
   \fancyhead[RE]{\nouppercase{\it\leftmark}}
   \fancyhead[LE,RO]{\it\thepage}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
% listings
\usepackage{listings}
\lstset{%
language=C++,%
commentstyle=\textit,%
identifierstyle=\textsf,%
keywordstyle=\sffamily\bfseries, %\texttt, %
%captionpos=b,%
tabsize=3,%
frame=lines,%
numbers=left,%
numberstyle=\tiny,%
numbersep=5pt,%
breaklines=true,%
morekeywords={descriptor_gaussian,descriptor,partition,fcm_possibilistic,dataset,my_exception,exception,std,vector},%
escapeinside={@*}{*@},%
%texcl=true, % wylacza tryb verbatim w komentarzach jednolinijkowych
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% TODO LIST GENERATOR %%%%%%%%%

\usepackage{color}
\definecolor{brickred}      {cmyk}{0   , 0.89, 0.94, 0.28}

\makeatletter \newcommand \kslistofremarks{\section*{Remarks} \@starttoc{rks}}
  \newcommand\l@uwagas[2]
    {\par\noindent \textbf{#2:} %\parbox{10cm}
{#1}\par} \makeatother


\newcommand{\remark}[1]{%
{%\marginpar{\textdbend}
{\color{brickred}{[#1]}}}%
\addcontentsline{rks}{uwagas}{\protect{#1}}%
}

%%%%%%%%%%%%%% END OF TODO LIST GENERATOR %%%%%%%%%%%

% some issues...

\newcounter{PagesWithoutNumbers}

\newcommand{\hcancel}[1]{%
    \tikz[baseline=(tocancel.base)]{
        \node[inner sep=0pt,outer sep=0pt] (tocancel) {#1};
        \draw[red] (tocancel.south west) -- (tocancel.north east);
    }%
}%

\newcommand{\MonthName}{%
  \ifcase\the\month
  \or January% 1
  \or February% 2
  \or March% 3
  \or April% 4
  \or May% 5
  \or June% 6
  \or July% 7
  \or August% 8
  \or September% 9
  \or October% 10
  \or November% 11
  \or December% 12
  \fi}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Helvetica font macros for the title page:
\newcommand{\headerfont}{\fontfamily{phv}\fontsize{18}{18}\bfseries\scshape\selectfont}
\newcommand{\titlefont}{\fontfamily{phv}\fontsize{18}{18}\selectfont}
\newcommand{\otherfont}{\fontfamily{phv}\fontsize{14}{14}\selectfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\Author}{Damian Kucharski}
\newcommand{\Supervisor}{Łukasz Wróbel, PhD}
\newcommand{\Title}{Automated gradient boosting framework for classification data}
\newcommand{\Polsl}{Silesian University of Technology}
\newcommand{\Faculty}{Faculty of Automatic Control, Electronics and Computer Science}


\begin{document}
	
%%%%%%%%%%%%%%%%%%  Title page %%%%%%%%%%%%%%%%%%%
\pagestyle{empty}
{
	\newgeometry{top=2.5cm,%
	             bottom=2.5cm,%
	             left=3cm,
	             right=2.5cm}
	\sffamily
	\rule{0cm}{0cm}
	
	\begin{center}
	\includegraphics[width=29mm]{polsl}
	\end{center}
	\vspace{1cm}
	\begin{center}
	\headerfont \Polsl
	\end{center}
	\begin{center}
	\headerfont \Faculty
	\end{center}
	\vfill
	\begin{center}
	\titlefont Engineer  thesis
	\end{center}
	\vfill
	
	\begin{center}
	\otherfont \Title\par
	\end{center}
	
	\vfill
	
	\vfill
	
	\noindent\vbox
	{
		\hbox{\otherfont author: \Author}
		\vspace{12pt}
		\hbox{\otherfont supervisor: \Supervisor}
	}
	\vfill

   \begin{center}
   \otherfont Gliwice,  \MonthName\ \the\year
   \end{center}	
	\restoregeometry
}


\cleardoublepage


\rmfamily
\normalfont


%%%%%%%%%%%% statements required by law and Dean's office %%%%%%%%%%
\cleardoublepage

\begin{flushright}
załącznik nr 2 do zarz. nr 97/08/09
\end{flushright}

\vfill

\begin{center}
\Large\bfseries Oświadczenie
\end{center}

\vfill

Wyrażam  zgodę / Nie wyrażam zgody*  na  udostępnienie  mojej  pracy  dyplomowej / rozprawy doktorskiej*.

\vfill

Gliwice, dnia {\selectlanguage{polish}\today}

\vfill

\rule{0.5\textwidth}{0cm}\dotfill

\rule{0.5\textwidth}{0cm}
\begin{minipage}{0.45\textwidth}
{\begin{center}(podpis)\end{center}}
\end{minipage}

\vfill

\rule{0.5\textwidth}{0cm}\dotfill

\rule{0.5\textwidth}{0cm}
\begin{minipage}{0.45\textwidth}
{\begin{center}\rule{0mm}{5mm}(poświadczenie wiarygodności podpisu przez Dziekanat)\end{center}}
\end{minipage}


\vfill

* podkreślić właściwe




% %%%%%%%%%%%%%%%%%%%%%
% \cleardoublepage

% \rule{1cm}{0cm}

% \vfill

% \begin{center}
% \Large\bfseries Oświadczenie promotora
% \end{center}

% \vfill

% Oświadczam, że praca „\Title” spełnia wymagania formalne pracy dyplomowej inżynierskiej.

% \vfill



% \vfill

% Gliwice, dnia {\selectlanguage{polish}\today}

% \rule{0.5\textwidth}{0cm}\dotfill

% \rule{0.5\textwidth}{0cm}
% \begin{minipage}{0.45\textwidth}
% {\begin{center}(podpis promotora)\end{center}}
% \end{minipage}

% \vfill



% \cleardoublepage


%%%%%%%%%%%%%%%%%% Table of contents %%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{Roman}
\pagestyle{onlyPageNumbers}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{PagesWithoutNumbers}{\value{page}}
\mainmatter
\pagestyle{PageNumbersChapterTitles}

%%%%%%%%%%%%%% body of the thesis %%%%%%%%%%%%%%%%%


\chapter{Introduction}

The roots of artificial intelligence and machine learning appear quite early in history, one of the earliest examples could be the introduction of the Bayes Theorem in the 1700s, followed by regression analysis in the 1800s. The field constantly evolved, but it got much more attention pretty recently when computational possibilities expanded enough for big data to be processed and more complex algorithms (like neural networks) to be used.
The thing that at first was called inferential statistics, data mining, and recently data science and machine learning has application in almost any field imaginable. Tools such as spam filters, chatbots, web search results, analysis of medical imaging, reconstruction of images, text generation and auto-completion, robotics, video, and board games, logistic processes optimization, and much more -- they all benefit from intelligent solutions that are built into them.
Machines can exceed the performance of humans in many critical fields, well-trained machine learning model can analyze mammography imaging as well or better than experienced radiologist \cite{inproceedings}. Very recently, in November 2020, a company called OpenAI, known for the creation of AlphaZero \cite{silver2017mastering} -- best chess playing program and AlphaGo \cite{alpha_go} -- the first-ever program to beat a human in the ancient Chinese game of Go, has created a program called AlphaFold2 \cite{alpha_fold}. It solves the task of so called protein folding, a 50-year-old grand challenge in biology, with a tremendous 90 percent accuracy, exceeding the performance of its first version from 2 years before by over 30 percent. This work by some is named the biggest accomplishment of artificial intelligence studies and is predicted to be a crucial part of the next great discoveries in medical and biology fields. Some applications, such as the evolutionary analysis of proteins, are set to flourish because the tsunami of available genomic data might now be reliably translated into structures. This can for example help to prevent such events like a covid-19 pandemic and help to find a cure for many diseases whose mechanisms are not yet well understood.
\newline
Occurrences like that show two things. First -- that the field of machine learning and artificial intelligence has and will have a great impact on the world, and second -- that the advancements in this field are very far from slowing down, in fact before mentioned AlphaFold and other novel machine learning models like transformers that pushed natural language modeling to the whole new level are the best evidence that the opposite is happening - this growth is speeding up.
\newline
However, with new ideas, growth of the availability of data, and processing power, the complexity of the models expanded and because of that, it is now much harder to make them perform optimally. It is much harder to break into the field now than it was just a few years ago. Ideas considered state-of-art are now mostly used only for educational purposes and even for experienced specialists it is much harder to create a model that achieves its full potential. The quality of the final model of the data depends mostly on two factors: 
\begin{itemize}
\item The quality and prepossessing of the data, feature engineering
\item Choosing right model and its hyperparameters.
\end{itemize}

Because of the reason for the growing complexity of the modeling task, the idea of automatic solution arose and is mostly referred to as Auto Modeling, Auto Machine Learning, or just Auto-ML. Such a solution should automate at least one part of the modeling pipeline, which means that it should either preprocess data for modeling purposes, select the right model and optimize its hyper-parameter choices, or to do both.
Such a solution gives a good baseline and pushes in the right direction, saving time otherwise spent for experiments that are destined to fail. In short words, Auto-ML is the idea to incorporate machine learning to create machine learning models. It is still a rather new field, however, it already gives promising results, and likely in a short amount of time it will completely replace a big chunk of traditional work performed by data scientists and other practitioners in the field, just as spreadsheets replaced the need for manual calculation on the paper for most of the use cases.
\newline
There are many approaches to solving such a problem, especially in terms of optimizing for model choice. 
Usually, the hardest part of the task is to optimize hyper-parameters. Most of them are different for each model type and while their optimal choice is crucial for good modeling of given data they can't be directly learned from it. For example for the \emph{k nearest neighbors algorithm} first, the number $k$ of neighbors have to be specified. For tree-based models, the maximal depth of the tree has to be provided. Almost every algorithm has its specific hyper-parameters. 
Another example of the hyperparameter is not directly model specific but necessary for the model to learn -- namely optimization method. What is important is that this method by itself can have its hyper-parameters. More details on different hyperparameters will be explored later in the work but for now what is important is that the more complex the model is, the more hyperparameters, which usually means a much harder procedure of finding their optimal values.
\newline
The objective of this thesis is the development of Auto-ML functionality in form of a python library. It should cover both data processing and model selection. Models built by it are based on gradient boosted trees algorithms that are optimized using a user's method of choice, implemented inside the library. Namely, grid search and Bayesian optimization are supported. Apart from that, a web-based application was developed that can be accessed using the internet or run on one's machine. It allows for training models and viewing visualizations summarizing training and explaining model decisions. As with the growing complexity of modern machine learning algorithms, it is increasingly hard to understand their decisions and it is very common to just treat them as a black box. However good understanding of the model's inner workings has proved in the past to be crucial in further enhancement of its performance. A good example can be the paper "Visualizing and Understanding Convolutional Networks" \cite{VisNeuralNet} by Matthew D Zeiler et. al. This study analyzed winning convolutional neural network architecture of previous ImageNet competition. As result, the next iteration of this competition has been won by this team of researchers.
Because of that software prepared to accompany this work has been designed to offer tools that make it easy to understand how the model works to make further research and domain knowledge application as simple as possible.
The library also offers a lower level interface, that gives a possibility to quickly ensemble machine learning pipelines for programmers without sacrificing the complexity of it. Because of that, it is much simpler to perform repetitive, standard tasks in data processing and modeling, at the same time enhancing the simplicity and readability of the code. Hyperparameters of such models can be then optimized with before mentioned methods.
The project was co-created with Arkadiusz Czerwiński who also ensured that regression tasks are well covered, whilst my scope was to prepare the library for handling classification problems. Most of the work, however, was done in collaboration as a big chunk of the whole code was needed to handle both use cases. The following chapters will analyze the classification problem, model selection, and hyper-parameter search, along with a description of the methods that are used to solve them. Then the proposed solution will be presented along with experiments, results, and conclusions.




\chapter{Problem analysis}

As was mentioned in the introduction, the Auto-ML library that is the subject of this thesis has to do both things -- preprocess the data and optimize hyperparameters. The created model should perform classification tasks with high efficiency, measured using metrics such as accuracy and f1-score.
After the training process, it should be possible to plot information concerning the effectiveness of training and model characteristics explanation.
All of these problems will be now explained so the solution presented by this thesis will be more understandable.

\section{Classification}

The main problem that the solution should solve is the so-called classification task. Such a task consists of predicting fixed class label for a given set of data. Classification is the most frequent problem in machine learning use cases. There are two types of classification problems - binary classification and multiclass classification.
In the first case model, given the data, one has to choose one of only two classes, usually representing True/False values or the presence/absence of something. Examples include classifying e-mails as spam and not-spam or predicting whether some person has some medical condition or not.
In the second case, there are at least 3 labels, usually predefined, and the model has to choose one of them that most likely categorizes the data it was presented. Depending on the algorithm used to perform the task, it can be either split into multiple binary classification problems using the so-called one-vs-all approach in which for each class individual problem of classifying if given data point belongs to a given class or not. However, some algorithms allow performing multiclass classification at one step. An example of such problems is for example famous MNIST \cite{deng2012mnist} classification task, in which given a picture of a handwritten digit, the program has to output what digit is present in the image.
To illustrate such a problem simple example will be provided. The problem to be solved will be the classification of student college acceptance given scores of two tests. Data is presented on the figure \ref{fig:class_task}.


\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.75]{college.png}
    \caption{Example of classification task}
    \label{fig:class_task}
\end{figure}


\subsection{Bias and Variance}

There are two important concepts that are necessary to be known to understand the whole model optimization, model selection, data processing, and others are about. They are called \emph{Bias} and \emph{Variance} and in a way, they measure the quality of the model.
The model is said to have a high bias if it is too simple to reproduce the relationships that are present in the data. Such a model is inherently incapable of representing them and therefore is highly biased. It is also often said that the model \emph{under-fits} the data. On the other side, there is a model that has a high variance. This situation occurs when the model is too complex and tries too hard to model relationships it sees in the data. Such a model may learn to make perfect predictions for the data it has seen but it would most likely completely fail to do it with the new data. It, therefore, has high variance because the relationships it models highly vary depending on the specific set of data points that were presented to it during the training process. Such a model is often said to \emph{over-fit} the data.
Ideally, the model should have low bias and low variance but it is often impossible because when one decreases other increases. The task then is to find a good middle-ground and the problem just referenced is often named \emph{Bias-Variance Tradeoff}. Examples of well and purely fitted models can be seen on the figure \ref{fitting}

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=1.5]{overfitting_2.png}
  \setcaptioncitation{\url{https://cutt.ly/4jaUM7Y}}
  \caption{Example of underfitting and overfitting}
  \label{fitting}
\end{figure}

\subsection{Classifiers, tree-based methods and gradient boosting}

There are many machine learning models that are suitable for use for classification tasks. First, there are simple ones like logistic regression \cite{wright1995logistic}. This model tries to fit the line, or in case of a problem located in higher dimensions -- hyperplane, that separates classes. Then there are more complicated models like support vector machines  \cite{noble2006support}. These were state-of-art for a long time until deep learning \cite{goodfellow2016deep} happened. Such a model tries to find a boundary that guarantees the biggest margin, in linear algebra sense, between classes. With the use of so-called kernel tricks and functions known as kernels, it can achieve impressive results. Then recently the deep learning era emerged and models known as neural networks became the best in many applications, they however usually require big amounts of data to achieve good results.
In this thesis however tree-based models \cite{buntine1992learning} were chosen to be the base of the solution. They have some qualities that make them an especially good choice for tackling the problem and some of these qualities as well as some theories of how the models' work will be shortly explained.

\subsubsection{Decision Tree and Gradient Boosting}

A decision tree is a simple model trying to learn simple decision rules inferred from the training data. Such rules can be easily visualized as a series of if-else statements. This behavior can be achieved with different methods however what each of them tries to do is finding optimal splits -- so decisions -- that explain differences between samples. The sequence of such rules creates a tree structure that can be easily interpreted. An example of such a decision tree can be found in figure \ref{fig:tree}.

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=0.4]{decision_tree.png}
  \setcaptioncitation{\url{https://cutt.ly/hh7jmin}}
  \caption{Visualization of decision tree}
  \label{fig:tree}
\end{figure}


To achieve better performance of the machine learning model there is a possibility to use a method called ensembling. Ensembling means training multiple machine learning models and then combining their predictions to get the final result. This approach often gives better results than using these models separately. 
One of the approaches that are based on the idea of ensembling is gradient boosting.
It works by creating many so-called weak learners and sequentially training them based on the results they give. The most often used components of such a method are small decision trees. Such trees are constrained on their maximum depth - so in other words the number of decisions (or splits) they can make. 
Because of that none of them can fit the data well but also it's impossible to overfit with such a model.
Then after creating the tree its error is measured and knowing that error and particular samples that model got especially wrong -- next tree tries to act on this error and correct it. This way every single model tries to "boost" the aggregate complexity of the ensemble. Such a process is repeated n times, where n is a hyperparameter. For every dataset, the optimal number of trees in the ensemble may be different and therefore should be optimized by experiments or other methods that will be described later and that are the core of the framework. This process is visualized in figure \ref{fig:boosting}  

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.75]{boosting.png}
    \setcaptioncitation{\url{https://cutt.ly/ijfUC4m}}
    \caption{Boosting algorithm}
    \label{fig:boosting}
\end{figure}

\subsection{Existing gradient boosting implementations}

\subsubsection{XGBoost}

XGBoost \cite{chen2016xgboost} is the most known library providing gradient boosting solutions. It was firstly created by Tianqi Chen and then continued by other creators. It is available for most popular operating systems - Windows, Linux, and macOS. Its implementations can be seen in many programming languages, commonly used in the field of data science and machine learning. Namely Python, C++, Julia, Java, Perl, and Scala support XGBoost. It is known for consistently good results and a reasonable run time of training.


\subsubsection{LightGBM}

The second framework is called LightGBM \cite{ke2017lightgbm} and as the name suggests it's the main goal is to be fast and lightweight. Thanks to that it can be used on not so powerful machines that do not possess great computing powers and big volumes of memory. While firstly created b Guolin Ke it was later incorporated as a Microsofts project. Its approach to growing the decision trees is also different from that of most of its competitors -- growing trees leaf-wise instead of tree-level-wise. It assumes that this approach usually yields the largest decrease in loss. Thanks to all these traits this library is very versatile and can be used almost anywhere and also it is great for quick prototyping and experimenting. 

\subsubsection{CatBoost}

Another framework is CatBoost \cite{prokhorenkova2018catboost}. It is newer than the former as it was first launched in the middle of the year 2017. While not that popular as other solutions it is certainly very interesting and powerful. It is successfully used in such applications as recommender systems, personal assistants, self-driving cars, weather prediction, and others. What is most remarkable in it is that it support GPU computing which greatly enhances the speed of training and also enables the possibility for creating more complex models on more complex data. It also has built-in visualization tools that make it easy to understand some insights that can be drawn from the training process and also to make tuning the algorithm easier. It also natively supports some preprocessing features like encoding categorical variables and missing values.

\subsection{Why gradient boosting?}

The choice of gradient boosting instead of other popular classification methods is not an accident. It has the most important traits for the auto-ml solution. It has proven to be effective many times. Kaggle \cite{bib:Kaggle} platform which is a place where machine learning specialists can participate in data science and machine learning competitions has a long record of gradient boosting and especially XGBoost being a primary tool that made people winners \cite{bib:Kaggle_survey}. 
It being tree-based also helps in model explainability which helps in the turn optimization procedure. The fact that the model can then be interpreted by humans more easily makes Auto-ML solutions more suitable for further improvements as it is easier to understand models' drawbacks and fix them.


\section{Data preprocessing}

Before the classification model can be trained, often the necessary step is to firstly preprocess it. Term preprocessing means performing transformations of the data such that the machine learning model of our choice can actually interpret and learn from it. Some of the most important will be now discussed.

\subsection{Missing value treatment}

Datasets are often incomplete. There are many reasons for that. Depending on the nature of the data, or the process that generated it, the cause for that may be different. There are many ways this problem can be addressed and different approaches are suitable in different situations.
There are three main ways missing values can be handled. The first one is just throwing away part of the data that is missing. It can be done in one of two ways. First is often used when some feature has mostly missing values. In such a situation one can just delete it entirely from the dataset. The other situation is often practiced when there are not many missing values of some feature. In such a situation the entries with undefined value for the feature can be deleted. This approach however deletes some potentially important information given by other features.
Another way to treat missing values is to impute them. Imputing means setting them to some values. Most popular approaches include for example setting them to the most frequent value for the feature, or if the values are numerical -- to mean or median. This approach however can increase the bias in the data, especially if the percentage of missing values is high. 
There are also some more complicated and smarter ways of imputing values like Iterative Imputation \cite{liu2013comparison} or KNN Imputation \cite{beretta2016nearest}. However, their mechanisms of work will are out of the scope of this thesis.
What is important however is that each of these methods, simple or not, has its advantages and disadvantages, and choosing the right one is more often a matter of experimenting rather than anything else.
The last method that will be discussed here is missing values encoding. This is the easiest method and it also is very effective in many cases. If the feature having missing values is nominal, or in other words, categorical what can be done is introducing a new class indicating that value is missing. In such a scenario if there is a binary "sick" feature in the dataset the category of "not known" can be added thus making the feature taking three possible values instead of two. If the data is numerical missing values can be set to zero and an additional binary feature called indicator variable can be added to the dataset, telling if there was a missing value in a given entry for the feature or not.

\subsection{Feature encoding}

Another necessary step for most of the cases is the so-called feature encoding. While humans are capable of processing words computers are not. Because of that, every feature in our dataset that is represented in for of the text has to be encoded somehow -- which basically means represented by numbers.
There are many ways to do it -- again the correct method depends on the data and its nature. 
In this thesis, only some of them with limited explanations will be covered as the topic is very broad.
The most natural way is to just assign a fixed number for each different value the feature can take. For example, if the "sick" variable could take values "YES" and "NO" then they could just be assigned to "1" and "0" respectively. This approach however very often is wrong. The problem arises when the number of possible values the feature can take is bigger than two. Encoding them in such a way in this case often could mean creating artificial order that does not exist but the model would interpret the data in the way that it does exist. For example, if there would be "color" feature in the data taking values "RED", "GREEN" and "BLUE" encoding them to "0", "1" and "2" would create a false relationship of BLUE being somehow more different, or "bigger" in respect to "RED" than "GREEN" is.
In such a case most often used method is so-called one-hot encoding. 
The method consists of replacing the given feature with n indicator variables for each possible value of the feature. What is important however that in the case of such encoding the problem known as perfect multicollinearity \cite{alin2010multicollinearity} arises. It can be easily solved however by just discarding one of the created indicator variables. This approach potentially can greatly increase the number of features in the dataset and potentially cause other problems.
The last method that will be covered here is target encoding where the values are set to the average of the target value for a given category. This approach is easy to implement and often gives good results but increases the risk of overfitting to the training data.

\subsection{Outlier treatment}

The last important topic that needs to be addressed is outlier treatment. It is often the case that the dataset will contain entries that differ significantly from other observations. Such a big difference can happen in features, target, or both. It is often hard to tell what is the cause of its existence, it may be an error of measurement or some actual important cause. What is important is that some models are very sensitive to outliers and tree-based methods are one of them. Trying to account for outliers can reduce the performance of the model on the majority of observations. Therefore outliers need special treatment.
As with missing values, we can delete them or impute them if it seems reasonable. This however deletes the information the can be carried by an outlier. If it is assumed that outlier may be natural it can also be treated separately as a whole different problem, by a new model that can then be combined with the model for "usual" observations to achieve better results.
There are many methods for determining whether an observation is an outlier or not, most of them are based on statistical methods like computing z-scores of observations. 
Finding and treating outliers correctly can be a big part of the model working well or bad and thus it should never be omitted, especially in cases when the machine learning model is sensitive to them. An example of such a situation can be seen in figure \ref{fig:outlier}

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1]{outlier.jpg}
    \setcaptioncitation{\url{https://cutt.ly/njfIUKa}}
    \caption{Linear regression is a model sensitive to outliers.}
    \label{fig:outlier}
\end{figure}





\subsection{Other data processing steps}

There are also other things that can be done with data to increase model performance. Some algorithms benefit from scaling and transforming numerical values so that they fall in a specific range or follow a specific distribution. Sometimes new variables can be created on the base of others. This process named feature engineering requires some creativity and often also domain knowledge. Sometimes artificially creating more observations can help, this process is called data augmentation. It is hard if possible at all to name all things that can be tried and that potentially can help. Experimenting and getting experience is a good chunk of knowing what to do, but the before-mentioned steps are most necessary to make the model work at all and are the challenge that the solution implemented as the subject of this thesis is concentrated to solve.


\section{Model optimization and evaluation}

After the preprocessing procedure model can be trained. Most of the algorithms do have some default settings. The ones used in this thesis are no exception. However, as was stated many times before the process of model optimization doesn't end in the optimization of its parameters. Before the model is trained it has to be set up to work in a specific way and these settings called hyperparameters also have to be optimized. Gradient boosting models while powerful are rather difficult to optimize in terms of their settings. That's why most people just stay with default parameters or try and fail with changing them.
That's why the auto-ml solution that is being proposed in this thesis heavily focuses on automating this process.

\subsection{Classification metrics}
To assess how well the model is performing the mathematical method for measuring it has to be created.
Such a method or methods are called metrics and they are used just for that.
In this thesis 2 specific metrics are used, the second one being calculated with another two. Formulas and explanations will be now shortly visited.

\subsubsection{Accuracy}
This is the easiest metric that can be used. It measures how many examples were correctly classified out of all examples provided.
Thus the formula looks as follows:

\begin{equation}
\label{eq:accuracy}
Accuracy = \frac{Correctly Classified}{Correctly Classified + IncorrectlyClassified}
\end{equation}


\subsubsection{Precision and Recall}

These two metrics are good for measuring how the algorithm performs when the number of entries for each class highly varies. For these formulas it is useful to introduce the following:
\newline
\newline
TP -- True-positive rate -- a fraction of correctly classified positive examples
\newline
TN -- True-negative rate -- a fraction of correctly classified negative examples
\newline
FP -- False-positive rate -- a fraction of incorrectly classified positive examples
\newline
FN -- False-negative rate -- a fraction of incorrectly classified negative examples
\newline
\newline

Where positive and negative examples mean correspondingly labels of 1 and 0 in binary classification problem.


Then the formulas for precision are as follows:

\begin{equation}
Precision = \frac{TP}{TP + FP}
\end{equation}

\begin{equation}
Recall = \frac{TP}{TP + FN}
\end{equation}

These formulas however work only for binary classification purposes. In case of problems with multiple classes the formulas have to be restructured. Method used in this project is so called \emph{macro-averaging}.
\newline
Macro-average precision score can be defined as the arithmetic mean of all the precision scores of different classes. Mathematically it can be defined in such a way:
\begin{equation}
Precision_{macro} = \frac{P_{class1} + P_{class2} + ... + P_{classn}}{n}
\end{equation}
Where $P_{classi}$ is precision calculated for $ith$ class.
\newline
Macro-average recall score can be defined as the arithmetic mean of all the recall scores of different classes. Mathematically it can be defined in such a way:
\begin{equation}
Recall_{macro} = \frac{R_{class1} + R_{class2} + ... + R_{classn}}{n}
\end{equation}
Where $R_{classi}$ is recall calculated for $ith$ class.
\subsubsection{F1-Score}

F1-score is a metric that is used to somehow average the Precision and Recall and provide a more balanced measure. This, along with accuracy, is a metric that is mostly used in this thesis. The formula for the F1-score presents itself as follows:

\begin{equation}
F1\mbox{-}score = 2 * \frac{Precision * Recall}{Precision + Recall}
\end{equation}

\subsection{Cross-Validation}

To tune model hyperparameters some way to measure how well the model is trained has to be set up. What is important in creating a good predictive model is to make it perform well not only on the data it is being trained on but also on the data it is yet to be used on.
Because of that different methods for testing model performance were proposed. One of the most used that is also the method chosen here is called cross-validation.
It consists of splitting the training dataset into k folds. The parameter has to be specified by the user but in practice 5 fold are mainly used. Each fold consists of 1/k of the dataset entries. Then the set is split to train and test sets k times. For each iteration one of the k folds is chosen to be a test set and all others together make a training set. 
The model is then trained on the training set and its performance is measured on the testing set using a specified metric.
There are different metrics that can be used to determine the quality of the classification model, most commonly accuracy is used as a metric which is basically the fraction of all examples that were classified correctly.
Sometimes the dataset is highly imbalanced and most of the observations fall to one class, while others appear rather rarely. In such a case most commonly used metrics include balanced accuracy score and F1 score.
When the training and testing process is performed k times the results are averaged and the performance is noted.
Knowing that performance it is then possible to try different approaches to data processing or hyperparameter choice. Repeatedly experimenting with these different approaches and measuring the quality of the result using cross-validation is a good way to build great machine learning models.

\subsection{Grid and Randomized Search}

To optimize hyperparameters of the model one can choose different approaches. The most commonly used are naive approaches like \emph{Grid Search and Randomized Search} \cite{bergstra2012random}.
These approaches are very similar and at first, require the user to specify all values that they want to be tested. That's the first drawback of the method as it is easy to miss some possibly good options. It also enforces the user to at least partially know what values may be a good fit.
Then the combinations of values provided options are tested. In the case of Grid Search, each combination is being tested. In the case of Randomized Search, random values are picked a selected number of times.
This approach is an example of an exhaustive search approach. It may be very slow and lives only in the scope of specifically provided options. 
It is however very easy to implement and if the user actually knows which values are likely to be correct and all they want is to check which one is best then this approach may be the best. This process is illustrated in figure \ref{fig:grid}
It isn't, however, the best option for an auto-ml solution that's why the work that is created here relies more on a different approach.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.25]{grid search.png}
    \setcaptioncitation{\url{https://cutt.ly/ejxGzLh}}
    \caption{Grid and randomized search}
    \label{fig:grid}
\end{figure}

\subsection{Bayesian Optimization}

Bayesian optimization \cite{snoek2012practical} is a probabilistic approach that is usually used for hyperparameter tuning. While the exact implementation and math that stays behind it are rather complex it is rather easy to understand the basic idea.
At first, users have to provide ranges that hyperparameters can take. This is already a big improvement from exhaustive search approaches as the specific values do not have to be specified. All that is needed are boundaries.
Then the algorithm tries to model the accuracy (or error) as a function of hyperparameters in space provided by the user. This function is in practice usually modeled using a probabilistic model known as \emph{Gaussian process} \cite{mackay1998introduction}.
As the real function is not known the algorithm tries to estimate it. Such an estimate is called a surrogate function and the goal is to maximize the probability the surrogate function is as close to being real as possible.
At first, a random guess is being taken and the model performance is being evaluated. Based on it another hyperparameter value is chosen, the model is again evaluated and this process is repeated a selected number of times or until there is no noticeable improvement between experiments.
There are different methods for choosing the hyperparameter values and they are called acquisition functions.
Most commonly used once are the probability of improvement and expected improvement. 
The first one chooses the values that the model thinks are most likely to give the improved result. The latter pick values that it thinks will give the biggest improvement but don't focus on the specific probability of getting it. This process is visualized in figures \ref{fig:bayes1} and \ref{fig:bayes2}.
Such a method is much more intelligent and reliable than exhaustive approaches. 
However, it has the main drawback that is the fact that it is not that easy to make use of parallel computing as the sequential choices depend on each other. 
This was not the case in the grid and randomized searches as the values to be checked are known in advance and experiments can be easily performed simultaneously.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.5]{bayes1.png}
    \setcaptioncitation{\url{https://cutt.ly/IjxGTcu}}
    \caption{Initial function}
    \label{fig:bayes1}
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.5]{bayes2.png}
    \setcaptioncitation{\url{https://cutt.ly/DjxGTS8}}
    \caption{Updated function}
    \label{fig:bayes2}
\end{figure}
\newpage
\section{Existing Auto-ML tools}

There is a fair number of Auto-ML tools that are already available on the market. All of them have their strengths and weaknesses. Some of them will be now shortly introduced to show why the solution provided here has a place in the world of already existing solutions.
In general, they can be divided into two categories. First being the tools available as the cloud service, second being tools for programmers in a form of for example libraries.
The biggest players in the cloud service market provided their solutions to be alternatives to the standard process of data preprocessing and model selection.
Among these are products like Google Auto-ML Tables \cite{bib:google}, Azure Auto-ML \cite{bib:azure} and Amazon Sagemaker \cite{bib:amazon}. The first one is targeted at users that do have a problem to solve but do not have the skillset needed to do it. The second one is targeted at engineers that do have some knowledge and want to make it easier for them to prepare good models. Its default user experience is based on jupyter notebook ecosystem so programming knowledge is required. The last one also targets engineers but it requires some knowledge about the AWS platform to use successfully. All these tools are however rather pricey and very in-house-platform focused which means that it is much harder to deploy models created by them outside their specific ecosystem.
As for the tools in form of software library among the most popular ones are AutoSklearn \cite{bib:autosklearn} and TPOT \cite{bib:TPOT}.
The first one is the easiest choice for most applications as it is built into the Sklearn library. However, it has very limited capabilities and can't be configured in any way. 
Another one, TPOT, is an interesting library as it relies on genetic programming to optimize model parameters. It yields promising results but also is very slow because of its genetic approach. What is very useful in this product is that it generates python code that replicates results so it is easy to analyze the steps. 

\chapter{Requirements and tools}

The tool that is the subject of the thesis should meet the following set of requirements.


\section{Requirements and use cases}

\subsubsection{Functional requirements:}

\begin{itemize}
    \item The tool should be available both as the python library to be used in code as well as in the form of the web application.
    \item User should be granted the possibility to choose the maximum number of iterations that algorithm can perform to limit the time it takes to get results.
    \item The tool should accept data in form of .csv files. 
    \item Optimized machine learning model should be possible to be easy downloaded in pickle format.
\end{itemize}
\subsubsection{Non-functional requirements:}
\begin{itemize}
    \item The tool should support the latest python version that also supports libraries that it is dependent on.
    \item The tool should work on the most popular operating system like Windows, Linux, and macOS.
    \item The interface of the web application should be designed in a way that it provides a good user experience for both desktop computers and mobile devices.
    \item Good documentation of the tool should be provided to make it easy to pick-up the tool.
\end{itemize}

The use-case diagram illustrating the possible usages of the system can be seen in figure \ref{fig:use_case}.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.6]{Diagramik.png}
    \caption{Use case diagram}
    \label{fig:use_case}
\end{figure}

As it can be seen three possible users were considered in the diagram. The first one is an experienced user who is capable of creating machine learning solutions. In this case, he will most likely benefit the most from using individual modules provided by the library. It will speed-up his preprocessing or model optimization steps allowing him for creating experiments and final models much easier and in a more readable way.
The other user is a programmer that does want to incorporate some machine learning into his project but his specialization is something else. An example can be a web developer who wants to incorporate some user analytics model into his project. He will then benefit most from the $Learner$ module that will allow him to quickly create the model inside his existing program.
The last user is a non-programmer who just wants to perform some machine learning task like classification. All he has is the data and all he wants are results and maybe some insights. In this situation, the web application would be the best fit because it requires minimal knowledge to be used.

\section{Tools used}

The following tools were useful while creating the product. 

\subsection{Visual studio code}

{V}isual {S}tudio {C}ode \cite{bib:VSC} is a lightweight open-source text editor made by Microsoft. Most of its functionalities come from extensions that can be installed on demand by the user. The most important feature that it provided was support for python programming language in form of syntax highlighting, IntelliSense and debugger.


\subsection{Jupyter notebook}

Jupyter \cite{bib:Jupyter} notebook is a very popular tool among machine learning engineers and researchers. It's an environment for running python code where user can split the code into cells that can be run individually. The state of each cell and the variables it initialized and changed are remembered individually. Therefore it is not necessary to run the whole code top-down after minor changes which are especially useful when working with big datasets and chunks of code that take a long time to run.
The solution also helps to make code and experiments more readable as it supports REPL and markdown rendering.

\subsection{Git and GitHub}

As the version control tool the most popular git was chosen while for the platform where the code was hosted online Github was chosen. These solutions were very useful to keep track of changes that were made a long time ago as the project is rather big. Also, the version control was especially useful in the case of this project as it was co-created with another person.

\subsection{Neptune}

Neptune \cite{bib:Neptune} is a platform helping to organize machine learning experiments. It offers many features from simply logging metrics to providing model artifacts, code snippets, model hyperparameters, or data used in the given experiment. Its setup is very easy as all that has to be done is to create an experiment on their website, copy its unique id, and then inside python code simple one-line instructions are used to log what users demands. It was used to organize experiments and benchmarks performed for the sake of this thesis.


\section{Existing packages used}


\subsection{Numpy and Pandas}

Numpy and pandas that are built on top of NumPy are popular python libraries for data manipulation.
They were used to load and store the data firstly provided in form of .csv files as well as formatting it to form useful for processing and training machine learning models.

\subsection{Scikit-learn}

Scikit-learn or sklearn is the most popular machine learning package for python. It provides a massive amount of tools for data processing as well as for model selection and validation.
In this thesis, almost all data processing is done with this library as well as cross-validation. Some of the most important solutions it provides that were used in this project consists of:

\begin{itemize}
    \item Some of the encoding capabilities
    \item Validation capabilities like train-test split and cross-validation
    \item Imputation capabilities
    \item Transformer interface for custom transformers creation
    \item Performance metrics evaluation
    \item Multi-thread grid search algorithm
    \item Pipeline interface
\end{itemize}

\subsection{Category Encoders}

Category encoders are open-source python libraries following sklearn style guidelines and aiming for extending existing sklearn functionalities in terms of encoding categorical features.
It provides the implementation for a less common but very powerful solution for the encoding of nominal variables.
These implementations are used in this thesis to provide an alternative for typical methods like one-hot encoding, which sometimes gives better results.

\subsection{Gradient boosting libraries}

Python implementations of popular gradient boosting libraries, namely \emph{XGBoost}, \emph{CatBoost} and \emph{LightGBM} were used in this thesis as a basis of modeling methods for the auto-ml solution.



\subsection{Scikit-optimize}

Scikit-optimize is another open-source python library following sklearn style guidelines and aiming for extending its functionality. It provides the implementation for more complicated model optimization techniques.
In the case of this thesis -- the implementation of the Bayesian optimization algorithm provided by it was used as a building block for the hyperparameter search functionality of the solution. 

\subsection{Hyperopt}

Hyperopt is a Python library for serial and parallel optimization. It is the most popular tool in that domain and is used as an alternative option for scikit-optimize. It is however more of a second choice solution as it is much simpler to incorporate the latter into the automatic system that was the subject of the thesis.

\subsection{Streamlit}

Streamlit is a python library that makes it very easy to deploy data science applications. It provides easy tools for creating applications in form of interactive dashboards. It has some limitations in terms of the features provided. However, it was not an obstacle in implementing the GUI of the created system. 



\chapter{External specification}

\section{Hardware and software requirements}


Hardware and software requirements are very low for this framework. What is needed is an operating system supporting python programming language in version 3.8.x and a computer with x86 64-bit CPU of Intel or AMD architecture. The framework was not tested on other workstations however it may work on them too.
Such limitations make it possible to use it on almost any hardware, the size of datasets however has to be taken into account.

\section{Installation procedure}

\subsection{Obtaining framework from github}

The project repository can be found under the following link: \url{https://github.com/CoInitialized/Ember}
There are two ways to obtain the latest version of the library.
\subsubsection{Cloning git repository}
If git version control is installed on the system the easiest way is to run the following command through CLI:
\begin{verbatim}
git clone https://github.com/CoInitialized/Ember.git
\end{verbatim}
The most recent version of the library should be then downloaded to the current working directory.
\subsubsection{Direct download}
It is also possible to download the zip package directly from Github.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.6]{github.PNG}
    \caption{Direct download}
    \label{fig:mesh1}
\end{figure}

The package can then be extracted in the directory of choice.


\subsection{Setting up virtual environment}

To be able to use library python and python package manager (pip) are needed.
Python can be obtained from \url{https://www.python.org/downloads/}. 
Other, and maybe more convenient way is to install Anaconda or Miniconda which are python distributions especially suited for data science purposes and they also come with the conda package manager and the conda environments that make it easy to manage dependencies. They can be found at: 
\url{https://www.anaconda.com/}

\subsection{Installing dependencies}

If anaconda distribution was installed the easiest way to install required dependencies is to create a conda environment with the help of the $.yml$ file provided in the repository. All that is needed to be done is to execute command \ref{lst:env_create} in the console.

\begin{lstlisting}[language=bash,caption={installation of anaconda environment from console}, label={lst:env_create}]
conda env create -f environment.yml
\end{lstlisting}

Alternatively, all dependencies can be installed via pip with command \ref{lst:pip_install}.  

\begin{lstlisting}[language=bash,caption={installation with pip}, label={lst:pip_install}]
pip install -r requirements.txt
\end{lstlisting}

However, a good idea would be to do it inside the virtual environment. The instructions for setting up a virtual environment can be found at \url{https://docs.python.org/3/tutorial/venv.html}

\section{Usage methods}

\subsection{Web Mode}

In order to run application in web mode user has to create local server by running command \ref{lst:starting_command} in shell:

\begin{lstlisting}[language=bash,caption={Running streamlit app}
\label{lst:starting_command}
]
streamlit run streamlit_app.py
\end{lstlisting}

Then the web browser should automatically launch GUI. If not then going to the address: \url{http://localhost:8501/} should be sufficient.

The application should welcome the user with the screen similar to the one presented in figure \ref{fig:welcome}

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.6]{welcome.png}
    \caption{Welcome screen}
    \label{fig:welcome}
\end{figure}

Next, we set the objective to classification and include some more iterations to reduce the chance of under-fitting the model. The result should look similar to the one presented in figure \ref{fig:set}

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.6]{set.png}
    \caption{Choosing options}
    \label{fig:set}
\end{figure}

Then after clicking browse files the following window of file choice appears.
After that step first few rows of the dataset will be displayed. An example is shown in figure \ref{fig:header}

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.6]{header.png}
    \caption{Dataset header}
    \label{fig:header}
\end{figure}

The dataset should have a column named "class" that will specify the target variable.
Finally, after clicking the "Start training" button the optimization process will begin. After some time results can be observed on the graph showing how each boosting algorithm was optimized. An example graph showing loss of decay of the algorithms can be observed in figure \ref{fig:wykres}.
On the y axis, the specific values of the loss are shown. They range from one to zero and represent the loss is defined as

\begin{equation}
Loss = 1 - Accuracy
\end{equation}

Accuracy being defined as in equation \ref{eq:accuracy}.
Losses based on different metrics are not supported but may be added in the future.
On the x axis values represent subsequent iterations of the optimization algorithm.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.6]{Wykres.png}
    \caption{Training results}
    \label{fig:wykres}
\end{figure}

Under it, the graph of feature importance's can be seen. They are shown as a horizontal bar chart. The higher the bar next to the specific feature name - the more this feature is considered important for the model to predict the target variable. An example of it can be observed in figure \ref{fig:feature} 

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{Feature.png}
    \caption{Feature importance plot}
    \label{fig:feature}
\end{figure}


Below the graph button allowing the user to download the model as a pickle file is present.
Pickle is a python data stream format that allows saving object state into a fail for further loading. In this way, trained models can be saved for further access.

The early stopping procedure helps to stop training the model if it does not make any improvements. If the user has more resources at their disposal and wishes to wait for a potentially long time this method may be a good choice. An example of the training graph in such a case can be seen in figure \ref{fig:early}.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.6]{Wykres2.png}
    \caption{Early stopping}
    \label{fig:early}
\end{figure}
\newpage
\subsection{AutoML library}

The library is also equipped with a fully automated $Learner$ class. It can be easily integrated into existing code and offers more customization options. Example usage of it is shown in the listing \ref{lst:autolearn}

\begin{lstlisting}[language=Python,caption={Using automatic learner},label={lst:autolearn}]
# Necessary imports
from ember.autolearn import Learner
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# Reading the dataset from csv file.
dataset_classification = r'datasets\classification\audiology.csv'
data = pd.read_csv(dataset_classification)
# Splitting the dataset to features and target
X, y = data.drop(columns = ['class']), data['class']
# Splitting datasets for training and testing parts
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 20)
# Creating automatic learner object
learner = Learner(objective='classification', X = X_train, y = y_train)
# Starting training procedure
learner.fit(cv = 5, optimizer = 'bayes', cat=False, speed=100)
# Model evaluation
results = learner.predict(X_test)
print(accuracy_score(y_test, results))
\end{lstlisting}
\subsection{Pipeline steps library}

The last method of usage is by the single sub-modules allowing for building machine learning pipelines in a simple but highly customizable way.
Example usage can be found in the listing \ref{lst:pipeline} that is availble in appendix.




\chapter{Internal specification}

\section{Concept of the system}

The concept of the system is to build a product that can be used both in form of the library and web application. Machine learning solutions have become more and more popular and a big goal of the project is to make them accessible to people on all levels of expertise. 
Because of that simple web application is available where the user can simply put their data and start a training model. On the other hand person with some programming knowledge can include an automatic learner library to make a more customizable model and if one has even more knowledge and skill then they can use specific sub-modules for enhancing their workflow.

\section{Class structure and resume of important classes}

The system consists of 11 main classes that are responsible for the work of the entire system and are visualized in figure \ref{fig:uml}

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.45]{UML.png}
    \caption{UML Class Diagram}
    \label{fig:uml}
\end{figure}
\newpage
\begin{itemize}
    \item \textbf{Preprocessor --} This is the most important class for preprocessing steps. It is used to connect individual data transformers to one pipeline. It has simple interface allowing for quick pipeline ensebling.
    
    \item \textbf{Dtype selector -- } This Class performs selection of given data type. Can be used with $preprocessor$ to create individual transformation pipelines for different data types.
    
    \item \textbf{Fraction Selector --}  Transformer selecting and returning only columns having less than the specified percentage of missing values. Useful for preventing overfitting by imputation and high dimensionality. Can be used with $Preprocessor$ to easily incorporate it into a pipeline.
    
    \item \textbf{General imputer --} This class provides a simple interface for missing data imputation. It allows to perform it with many different methods like \emph{mean/mode imputation} or {KNN imputation}. Can be used with $Preprocessor$ to easily incorporate it into the pipeline.
    
    \item \textbf{GeneralScaler --}  This class provides a simple interface for numerical data scaling and normalization. Can be used with $Preprocessor$ to easily incorporate it into the pipeline.
    
    \item \textbf{MultiColumnTransformer --}  General transformer class for many columns. Used to fit the same transformer for each column.
    
    \item \textbf{General Encoder --}  This class provides a simple interface for categorical data encoding. It allows to perform it with many different methods like \emph{one-hot encoding}, {target encoding} or {leave-one-out encoding}. Can be used with $Preprocessor$ to easily incorporate it into the pipeline.
    
    \item \textbf{Selector --}  Abstract class for model hyperparameter optimization, implementing basic functions like $predict$ or $score$
    
    \item \textbf{GridSelector --}  Class made for model selection based on grid search and cross validation
    
    \item \textbf{BayesSelector --}  Class made for model selection based on bayesian optimization and cross validation
\end{itemize}

\section{AutoML pipeline steps implementation}
The automatic learner class created for this project performs both data processing and model optimization steps. The steps performed in both cases will be now shortly explained.
\subsection{Data preprocessing}
Data preprocessing is started by dropping the columns with a big chunk of missing entries. By default, it is set to 80 percent. This process can be stopped on-demand or restricted to only some columns if the user believes that missing values at some columns should be specifically indicated. In this case, indicator variables are created for missing values.
The next step is missing values imputation. By default it is performed with $mean/mode$ imputation. This method can also be chosen by others by the user. It also can be automatically selected individually for each feature by automatic experiments but it greatly extends runtime.
Then categorical values are encoded. By default, it is performed with target encoding but the user can specify another method. It also can be automatically selected individually for each feature by automatic experiments but it greatly extends runtime.

\subsection{Model optimization}

Model optimization can be performed with either bayesian optimization or grid selection. By default bayesian optimization is performed with the help of a scikit-optimize package implementing the optimization algorithm with gaussian processes. 
Users can change it to Bayesian optimization with hyperopt engine for optimization (based on tree parzen estimation method) or grid selection.



The whole process is visualized in the flowchart \ref{fig:flowchart}


\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.45]{flowchart.png}
    \caption{Flowchart of the system}
    \label{fig:flowchart}
\end{figure}

\chapter{Verification and validation}


As the main goal of the project was to prepare the python module that would optimize the commonly used gradient boosting framework the testing part focused on running it on some benchmark classification datasets.
Data were preprocessed using preprocessing modules created as part of the library and then LightGBM, XGBoost, and Catboost models were trained on default parameters. Along with them, the optimization procedure was run based on grid approach and Bayesian optimization which is the proposed optimization method. The default models were not only scored on the test datasets but their training accuracy was also measured using cross-validation scoring.
The datasets and their characteristics can be observed in table \ref{tab:datasety}.
 Results of the experiments performed on the benchmark datasets can be found in the table \ref{tab:benchmarki}. Each entry shows achieved test accuracy (equation \ref{eq:accuracy}) for a given dataset with a given method.
 Accuracy of $1$ means perfect accuracy where the accuracy of $0$ means no correct classification.  The mean and median of these scores can be found in table \ref{tab:mean_median_scores}
After a quick analysis, it can be seen that the optimized algorithms performed as well or better than default algorithms in $71\%$ of datasets. 
While it is a good result the further analysis was performed to determine why sometimes optimized algorithms gave worse results than default ones.
The train cross-validation score of default algorithms and the optimized ones were therefore compared. This comparison is visible in the table \ref{tab:crossy}.
As it can be seen the cross-validation score of the optimized algorithm is always higher than the one of default algorithms. It means that the algorithm optimized its objective correctly every time but it overfitted the validation sets even though cross-validation was used. The mean and median of these cross-validation scores can be observed in table \ref{tab:mean_median_cross}.
Therefore finally the results on the datasets where it was possible to optimize the algorithms without overfitting were performed. Results can be observed in table \ref{tab:lepsze}. As it can be seen in this situation the results are very good and the optimized algorithms greatly outperform default ones.


\begin{table}[!htb]
\centering



\begin{tabular}{lrrrr}

\toprule
{} &  \# rows &  \# columns &  Majority class perc. &  \# classes \\
Dataset name            &         &            &                                        &            \\
\midrule
anneal                  &     898 &         39 &                                   0.76 &          5 \\
auto-mpg                &     398 &          8 &                                   0.63 &          3 \\
balance-scale           &     625 &          5 &                                   0.46 &          3 \\
breast-cancer           &     286 &         10 &                                   0.70 &          2 \\
breast-w                &     699 &         10 &                                   0.66 &          2 \\
bupa-liver-disorders    &     345 &          7 &                                   0.58 &          2 \\
car                     &    1728 &          7 &                                   0.70 &          4 \\
cleveland               &     303 &         14 &                                   0.54 &          5 \\
credit-a                &     690 &         16 &                                   0.56 &          2 \\
credit-g                &    1000 &         21 &                                   0.70 &          2 \\
cylinder-bands          &     540 &         36 &                                   0.58 &          2 \\
diabetes                &     768 &          9 &                                   0.65 &          2 \\
echocardiogram          &     131 &         12 &                                   0.67 &          2 \\
ecoli                   &     336 &          8 &                                   0.43 &          8 \\
flag                    &     194 &         29 &                                   0.47 &          4 \\
glass                   &     214 &         10 &                                   0.36 &          6 \\
hayes-roth              &     132 &          5 &                                   0.39 &          3 \\
heart-c                 &     303 &         14 &                                   0.54 &          2 \\
heart-statlog           &     270 &         14 &                                   0.56 &          2 \\
hepatitis               &     155 &         20 &                                   0.79 &          2 \\
horse-colic             &     368 &         23 &                                   0.63 &          2 \\
hungarian-heart-disease &     294 &         14 &                                   0.64 &          2 \\
ionosphere              &     351 &         35 &                                   0.64 &          2 \\
iris                    &     150 &          5 &                                   0.33 &          3 \\
labor                   &      57 &         17 &                                   0.65 &          2 \\
lymph                   &     148 &         19 &                                   0.55 &          4 \\
mammographic-masses     &     961 &          6 &                                   0.54 &          2 \\
prnn-synth              &     250 &          3 &                                   0.50 &          2 \\
seismic-bumps           &    2584 &         19 &                                   0.93 &          2 \\
sonar                   &     208 &         61 &                                   0.53 &          2 \\
soybean                 &     683 &         36 &                                   0.13 &         19 \\
tic-tac-toe             &     958 &         10 &                                   0.65 &          2 \\
titanic                 &    2201 &          4 &                                   0.68 &          2 \\
vehicle                 &     846 &         19 &                                   0.26 &          4 \\
vote                    &     435 &         17 &                                   0.61 &          2 \\
wine                    &     178 &         14 &                                   0.40 &          3 \\
yeast                   &    1484 &          9 &                                   0.31 &         10 \\
zoo                     &     101 &         17 &                                   0.41 &          7 \\
\bottomrule
\end{tabular}
\caption{Datasets and their characteristics}
\label{tab:datasety}
\end{table}

\begin{table}[!htb]
\centering
\begin{tabular}{lrrrrr}
\toprule
{} &  LightGBM &  XGBoost &  CatBoost &  Bayes &  Grid \\
\midrule
anneal                  &  0.99 &     \textbf{1.00} &      \textbf{1.00} &   \textbf{1.00} &  \textbf{1.00} \\
auto-mpg                &  0.83 &     0.82 &      \textbf{0.84} &   0.82 &  0.82 \\
balance-scale           &  0.86 &     0.87 &      0.87 &   \textbf{0.99} &  0.91 \\
breast-cancer           &  0.69 &     0.72 &      0.71 &   \textbf{0.73} &  0.70 \\
breast-w                &  0.95 &     0.95 &      \textbf{0.96} &  \textbf{ 0.96} &  \textbf{0.96} \\
bupa-liver-disorders    &  \textbf{0.80} &     0.75 &      0.78 &   0.77 &  0.75 \\
car                     &  0.99 &     0.96 &      0.98 &   0.99 &  \textbf{1.00} \\
cleveland               &  0.49 &     0.55 &      0.53 &   \textbf{0.60} &  0.57 \\
credit-a                &  0.85 &     0.85 &      \textbf{0.86} &  \textbf{ 0.86} &  \textbf{0.86} \\
credit-g                &  0.73 &     0.74 &      \textbf{0.75} &   0.74 &  0.74 \\
cylinder-bands          &  0.81 &     0.82 &      \textbf{0.83} &   \textbf{0.83} &  0.81 \\
diabetes                &  0.74 &     \textbf{0.76} &      \textbf{0.76} &   0.75 &  0.74 \\
echocardiogram          &  0.88 &     0.85 &      0.90 &   0.93 &  \textbf{0.95} \\
ecoli                   &  0.83 &     0.82 &      0.85 &   0.85 &  \textbf{0.86} \\
flag                    &  0.68 &    \textbf{ 0.69} &      0.66 &   0.68 &  \textbf{0.69} \\
glass                   &  0.75 &     0.75 &      \textbf{0.78} &   0.75 &  0.75 \\
hayes-roth              &  0.68 &     0.75 &      0.78 &   \textbf{0.80} &  0.75 \\
heart-c                 &  0.77 &     0.77 &      0.79 &   0.78 &  \textbf{0.82} \\
heart-statlog           &  0.86 &     0.85 &      0.85 &  \textbf{ 0.90} &  0.88 \\
hepatitis               &  0.83 &     0.83 &      \textbf{0.85} &   \textbf{0.85} &  0.83 \\
horse-colic             &  0.91 &     0.91 &      \textbf{0.92} &   0.90 &  0.87 \\
hungarian-heart-disease &  0.76 &     0.79 &      \textbf{0.84} &   0.81 &  0.79 \\
ionosphere              &  \textbf{0.94} &     \textbf{0.94} &      \textbf{0.94} &   \textbf{0.94} &  \textbf{0.94} \\
iris                    &  0.89 &     0.93 &      0.89 &  \textbf{ 0.93} &  0.87 \\
labor                   &  0.67 &     0.89 &      \textbf{1.00} &   0.89 &  \textbf{1.00} \\
lymph                   &  0.84 &     0.89 &      0.82 &   \textbf{0.91} &  0.84 \\
mammographic-masses     &  0.80 &     0.82 &      0.82 &   \textbf{0.84} &  0.82 \\
prnn-synth              &  0.91 &     0.91 &      0.92 &   0.92 &  \textbf{0.93} \\
seismic-bumps           &  \textbf{0.94} &     0.93 &      \textbf{0.94} &   0.93 &  0.93 \\
sonar                   &  0.81 &     0.83 &      0.84 &   \textbf{0.86} &  0.86 \\
soybean                 &  \textbf{0.94} &     0.92 &      0.92 &   0.93 &  0.91 \\
tic-tac-toe             &  0.99 &     0.95 &      0.99 &   \textbf{1.00} &  0.99 \\
titanic                 &  \textbf{0.78} &     \textbf{0.78} &      \textbf{0.78} &   \textbf{0.78} &  0.77 \\
vehicle                 &  \textbf{0.80} &     \textbf{0.80} &      0.78 &   0.79 &  0.78 \\
vote                    &  \textbf{0.98} &     \textbf{0.98} &      \textbf{0.98} &   \textbf{0.98} &  0.97 \\
wine                    &  0.96 &     \textbf{1.00} &      0.98 &   \textbf{1.00} &  0.96 \\
yeast                   &  0.60 &     0.60 &      \textbf{0.61} &   0.60 &  0.59 \\
zoo                     &  0.55 &     0.74 &      \textbf{0.77} &   \textbf{0.77} &  \textbf{0.77} \\
\bottomrule
\end{tabular}
\caption{Accuracy on benchmark datasets}
\label{tab:benchmarki}
\end{table}


\begin{table}[!htb]
\centering
\begin{tabular}{lrr}
\toprule
{} &  Mean Value &  Median Value \\
\midrule
LGBM     &        0.82 &          0.83 \\
XGBoost  &        0.83 &          0.83 \\
CatBoost &        0.84 &          0.85 \\
Bayes    &        0.85 &          0.85 \\
Grid     &        0.84 &          0.85 \\
\bottomrule
\end{tabular}

\caption{Mean and median accuracy's of different algorithms}
\label{tab:mean_median_scores}
\end{table}


\begin{table}[!htb]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  LightGBM &  CatBoost &  XGBoost &  Optimized Model\\
\midrule
auto-mpg                &              0.80 &             0.82 &             0.82 &                   \textbf{0.85} \\
bupa-liver-disorders    &              0.71 &             0.74 &             0.74 &                   \textbf{0.77} \\
credit-g                &              0.75 &             0.77 &             0.75 &                   \textbf{0.76} \\
diabetes                &              0.71 &             0.74 &             0.74 &                   \textbf{0.77} \\
flag                    &              0.70 &             0.70 &             0.73 &                   \textbf{0.75} \\
glass                   &              0.72 &             0.73 &             0.69 &                   \textbf{0.77} \\
heart-c                 &              0.79 &             0.83 &             0.83 &                   \textbf{0.84} \\
horse-colic             &              0.81 &             0.81 &             0.79 &                   \textbf{0.84} \\
hungarian-heart-disease &              0.82 &             0.83 &             0.80 &                   \textbf{0.85} \\
labor                   &              0.64 &             0.90 &             0.85 &                   \textbf{0.90} \\
seismic-bumps           &              0.92 &             0.93 &             0.93 &                   \textbf{0.93} \\
soybean                 &              0.92 &             0.92 &             0.91 &                   \textbf{0.93} \\
vehicle                 &              0.76 &             0.76 &             0.75 &                   \textbf{0.77} \\
yeast                   &              0.58 &             0.58 &             0.60 &                   \textbf{0.61} \\
\bottomrule
\end{tabular}
\caption{Cross validation accuracy's in datasets for which optimization failed}
\label{tab:crossy}
\end{table}

\begin{table}[!htb]
\centering
\begin{tabular}{lrr}
\toprule
{} &  Mean Value &  Median Value \\
\midrule
LightGBM      &        0.76 &          0.75 \\
CatBoost       &        0.79 &          0.79 \\
XGBoost     &        0.78 &          0.77 \\
Optimized Model &        0.81 &          0.80 \\
\bottomrule
\end{tabular}

\caption{Mean and median cross validation accuracy's of different algorithms}
\label{tab:mean_median_cross}
\end{table}

\begin{table}[!htb]
\centering
\begin{tabular}{lrr}
\toprule
{} &  Mean accuracy &  Median accuracy \\
\midrule
LGBM     &        0.84 &          0.85 \\
XGBoost  &        0.86 &          0.85 \\
CatBoost &        0.86 &          0.86 \\
Bayes    &        0.89 &          0.91 \\
Grid     &        0.87 &          0.86 \\
\bottomrule
\end{tabular}


\caption{Mean and median accuracies on datasets where optimization succeeded}
\label{tab:lepsze}
\end{table}



As it can be seen the proposed solution based on Bayesian optimization correctly optimizes the objective which in this case was classification accuracy. However, it happens that even though cross-validation is used the optimization process sometimes leads to overfitting. This is apparent from table \ref{tab:crossy}.
In this case, hyperparameter optimization alone is not a solution and another step should be performed. It is also possible that other, simpler algorithms may outperform these based on gradient boosting. Such experiments are however outside the scope of this thesis and were not performed.
It is also very much possible that the optimization would not lead to overfitting if the datasets were preprocessed more carefully. This process however is only indirectly connected with the automatic search for gradient boosting frameworks hyperparameters and was covered very briefly in this project. 
The spread of scores that algorithms achieved overall datasets is visualized in the figure \ref{fig:box} using the box plot. The line that divides the box into 2 parts represents the median of the data. The end of the box shows the upper and lower quartiles. The extreme lines show the highest and lowest value excluding outliers. The floating points over and under the box are representing outliers.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1]{box.png}
    \caption{Algorithms performance box plots}
    \label{fig:box}
\end{figure}

\chapter{Conclusions}
Machine learning solutions based on gradient boosting are one of the best available. They are not well suited for unstructured data where deep neural networks usually perform best. However for structured data, they usually completely outperform other algorithms and they do not need huge data sets that are often the case with deep learning. The training time for these algorithms is also usually very reasonable. They are also much easier to interpret than other methods especially if used with tree-based models as in the case of these projects.

During the development of the project, all objectives of the thesis were realized. 
As a result fully functional library along with a web interface, allowing for automatic creation of machine learning pipelines and model optimization was created.
The methods used in the development of this project are considered state-of-art and very modern. Thanks to that the project is much more likely to be useful and relevant for a long time.
Thanks to clear documentation projects are very suitable for further development and open-source friendly. Many performance tests were performed which gave many insights on how well the project performs and what can be optimized.
Along with the tests, many ideas for additional useful features came up. The library could definitely benefit from better-preprocessing functionalities like tools for outlier detection and treatment or feature selection.
It was also found that the models usually do not perform well on highly imbalanced datasets. In such a case it would be useful to incorporate some additional tools for oversampling and data augmentation like SMOTE \cite{chawla2002smote}. The possibility for optimizing different objectives than accuracy would be a good addition too.
The overall project seems to have great potential. Its data preprocessing capabilities make it much easier to perform quick experiments that greatly reduces the time for the entry phase of the project. The automatic preprocessing functionalities however can be greatly optimized. Model optimization capabilities make it much easier to select the right model for the job. Because of the fact that choosing the right model cannot be done in a different way than by experimenting, automating this process at least to some extent is actually a very big deal.
Summarizing the project is definitely useful which was proven experimentally. It can and definitely will be further developed and improved.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter
\pagenumbering{Roman}
\stepcounter{PagesWithoutNumbers}
\setcounter{page}{\value{PagesWithoutNumbers}}

\pagestyle{onlyPageNumbers}

%%%%%%%%%%% bibliography %%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{bibliography}

%%%%%%%%%  appendices %%%%%%%%%%%%%%%%%%%

\begin{appendices}




\chapter*{List of abbreviations and symbols}


\begin{itemize}
   
 \item  FP -- False Positive

 \item  FN -- False Negative

 \item  TP -- True Positive

 \item  TN -- True Negative

 \item  Auto-ML -- Automatic Machine Learning

\end{itemize}

\chapter*{Listings}
\label{lst:pipeline}
\lstinputlisting[language = Python, caption = Using pipeline steps]{example_1.py}
\lstinputlisting[language = Python, caption = Code used for testing]{benchmark_classification.py}
\chapter*{Contents of attached CD}

The thesis is accompanied by a CD containing:
\begin{itemize}
\item thesis (\LaTeX\ source files and final \texttt{pdf} file),
\item source code of the application,
\item test data.
\end{itemize}


\listoffigures
\listoftables
	
\end{appendices}


\end{document}


%% Finis coronat opus. 
